#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML-–º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú
–ü–û–õ–ù–û–°–¢–¨–Æ –ü–†–û–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –°–¢–£–î–ï–ù–¢–û–í
"""

# =============================================================================
# –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö –ò –ù–ê–°–¢–†–û–ô–ö–ê –û–ö–†–£–ñ–ï–ù–ò–Ø
# =============================================================================

import subprocess
import sys
import os
import tempfile
import base64
from datetime import datetime

def install_and_import_packages():
    """
    –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ô –£–°–¢–ê–ù–û–í–ö–ò –ò –ò–ú–ü–û–†–¢–ê –ù–ï–û–ë–•–û–î–ò–ú–´–• –ü–ê–ö–ï–¢–û–í
    –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –æ–Ω–∏ –º–æ–≥–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–¥ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    """
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–∞–∫–µ—Ç–æ–≤
    temp_dir = tempfile.mkdtemp()
    print(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–æ–≤: {temp_dir}")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–∞–∫–µ—Ç–æ–≤ –≤ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    env = os.environ.copy()
    env['PYTHONUSERBASE'] = temp_dir
    
    # –°–ª–æ–≤–∞—Ä—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤: –∏–º—è_–ø–∞–∫–µ—Ç–∞: –≤–µ—Ä—Å–∏—è_–¥–ª—è_pip
    required_packages = {
        'numpy': 'numpy==1.21.0',      # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        'matplotlib': 'matplotlib'      # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    }
    
    # –ü–æ–ø—ã—Ç–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–∞–∂–¥–æ–≥–æ –ø–∞–∫–µ—Ç–∞
    for package_name, pip_name in required_packages.items():
        try:
            __import__(package_name)  # –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
            print(f"‚úì {package_name} —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        except ImportError:
            print(f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ {package_name}...")
            try:
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ pip –≤ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                subprocess.check_call([
                    sys.executable, "-m", "pip", "install",
                    "--user", "--no-cache-dir", "--no-warn-script-location",
                    pip_name
                ], env=env)
                print(f"‚úì {package_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except subprocess.CalledProcessError as e:
                print(f"‚úó –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {package_name}: {e}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
    possible_paths = [
        os.path.join(temp_dir, 'lib', 'python3.10', 'site-packages'),
        os.path.join(temp_dir, 'lib', 'python3.9', 'site-packages'),
        os.path.join(temp_dir, 'lib', 'python3.8', 'site-packages'),
        os.path.join(temp_dir, 'lib', 'python3.7', 'site-packages'),
        os.path.join(temp_dir, 'lib', 'python', 'site-packages'),
        os.path.join(temp_dir, 'site-packages'),
    ]
    
    for path in possible_paths:
        if os.path.exists(path) and path not in sys.path:
            sys.path.insert(0, path)
            print(f"–î–æ–±–∞–≤–ª–µ–Ω –ø—É—Ç—å: {path}")

# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–∞–∫–µ—Ç–æ–≤
install_and_import_packages()

# =============================================================================
# –ò–ú–ü–û–†–¢ –û–°–ù–û–í–ù–´–• –ë–ò–ë–õ–ò–û–¢–ï–ö –î–õ–Ø ML –ò –ê–ù–ê–õ–ò–ó–ê –î–ê–ù–ù–´–•
# =============================================================================

try:
    # PySpark - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    from pyspark.sql import SparkSession
    from pyspark.sql import functions as F  # –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DataFrame
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, FloatType, IntegerType
    from pyspark.ml.feature import VectorAssembler, StringIndexer  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    from pyspark.ml.classification import LogisticRegression  # –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
    from pyspark.ml import Pipeline  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è ML-–ø–∞–π–ø–ª–∞–π–Ω–æ–≤
    
except ImportError as e:
    print(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ PySpark: {e}")
    sys.exit(1)

try:
    # NumPy - —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    import numpy as np
    print("‚úì NumPy —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
except ImportError:
    print("‚úó NumPy –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É")
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –µ—Å–ª–∏ NumPy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    class MockNumpy:
        def randint(self, max_val):
            import random
            return random.randint(0, max_val-1)
    np = MockNumpy()

try:
    # Matplotlib - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    import matplotlib.pyplot as plt
    print("‚úì Matplotlib —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
except ImportError:
    print("‚úó Matplotlib –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞")
    plt = None

# =============================================================================
# –§–£–ù–ö–¶–ò–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ò –ú–û–î–ï–õ–ò (–û–ë–™–Ø–°–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í)
# =============================================================================

def interpret_model(model, feature_cols):
    """
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        feature_cols: —Å–ø–∏—Å–æ–∫ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    Returns:
        feature_importance: –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø—Ä–∏–∑–Ω–∞–∫, –≤–∞–∂–Ω–æ—Å—Ç—å)
    """
    print("5. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏ (–≤–µ—Å–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    coefficients = model.coefficients
    intercept = model.intercept  # –°–≤–æ–±–æ–¥–Ω—ã–π —á–ª–µ–Ω

    print(f"Intercept (—Å–≤–æ–±–æ–¥–Ω—ã–π —á–ª–µ–Ω): {intercept:.4f}")
    print("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏):")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã (–ø—Ä–∏–∑–Ω–∞–∫, –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç)
    feature_importance = list(zip(feature_cols, coefficients))

    # –í—ã–≤–æ–¥–∏–º –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
    for f, c in feature_importance:
        print(f"  {f}: {float(c):.6f}")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
    feature_importance = [(str(f), float(c)) for f, c in feature_importance]
    feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)

    return feature_importance

# =============================================================================
# –§–£–ù–ö–¶–ò–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –í S3
# =============================================================================

def save_business_artifacts_fixed(spark, df_ml, feature_importance, metrics, business_insights):
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ S3
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    
    Args:
        spark: SparkSession
        df_ml: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        feature_importance: –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        metrics: –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        business_insights: –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã
    """
    print("üíº –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤ S3...")
    
    # –°–æ–∑–¥–∞–µ–º –ø—É—Ç—å —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    current_date = datetime.now().strftime("%Y-%m-%d")
    base_path = f"s3a://bucket-ml/reports/{current_date}"
    
    try:
        # 1. –°–û–•–†–ê–ù–ï–ù–ò–ï –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í –í CSV
        feature_data = []
        for feat, imp in feature_importance:
            feature_data.append((str(feat), float(imp), float(abs(imp))))
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—É –¥–ª—è DataFrame
        feature_schema = StructType([
            StructField("feature", StringType(), True),
            StructField("coefficient", DoubleType(), True),
            StructField("absolute_importance", DoubleType(), True)
        ])
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        feature_df = spark.createDataFrame(feature_data, schema=feature_schema)
        feature_df = feature_df.orderBy("absolute_importance", ascending=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
        feature_df.write \
            .mode("overwrite") \
            .option("header", "true") \
            .option("delimiter", ";") \
            .csv(f"{base_path}/feature_importance/")
        
        print("   ‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ CSV")
        
        # 2. –°–û–•–†–ê–ù–ï–ù–ò–ï –ë–ò–ó–ù–ï–°-–ò–ù–°–ê–ô–¢–û–í
        insights_text = f"""–ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ - –û—Ç—á–µ—Ç –æ—Ç {current_date}

{business_insights}

---
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ML-–ø–∞–π–ø–ª–∞–π–Ω–æ–º
"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        insights_df = spark.createDataFrame([(insights_text,)], ["content"])
        insights_df.coalesce(1).write \
            .mode("overwrite") \
            .text(f"{base_path}/business_insights/")
        
        print("   ‚úÖ –ë–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        
        # 3. –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –í –§–û–†–ú–ê–¢–ï JSON
        churned_count = df_ml.filter(F.col("is_churned") == 1).count()
        total_count = df_ml.count()
        churn_rate = float(churned_count / total_count) if total_count > 0 else 0.0
        
        metrics_summary = f"""{{
    "report_date": "{current_date}",
    "model_metrics": {{
        "auc_score": {float(metrics['auc'])},
        "f1_score": {float(metrics['f1'])}
    }},
    "business_metrics": {{
        "total_customers_analyzed": {total_count},
        "churned_customers": {churned_count},
        "churn_rate": {churn_rate:.4f}
    }}
}}"""
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∫ —Ç–µ–∫—Å—Ç
        metrics_df = spark.createDataFrame([(metrics_summary,)], ["content"])
        metrics_df.coalesce(1).write \
            .mode("overwrite") \
            .text(f"{base_path}/metrics_summary/")
        
        print("   ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON")
        
        # 4. –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–†–ò–ú–ï–†–ê –î–ê–ù–ù–´–• –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê
        sample_data = df_ml.select(
            "customer_id", "is_churned", "avg_tx_amount", 
            "total_tx_count", "days_since_last_tx", "region"
        ).limit(1000)
        
        sample_data.write \
            .mode("overwrite") \
            .option("header", "true") \
            .option("delimiter", ";") \
            .csv(f"{base_path}/sample_data/")
        
        print("   ‚úÖ –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        
        return base_path
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {e}")
        return base_path

# =============================================================================
# –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò HTML –û–¢–ß–ï–¢–ê –° –†–£–°–°–ö–ò–ú–ò –ù–ê–ó–í–ê–ù–ò–Ø–ú–ò –ü–†–ò–ó–ù–ê–ö–û–í
# =============================================================================

def generate_html_report(spark, feature_importance, metrics, business_insights, save_path):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ HTML –æ—Ç—á–µ—Ç–∞ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏
    –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –ø–æ–Ω—è—Ç–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    """
    print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞...")
    
    if plt is None:
        print("   ‚ö†Ô∏è Matplotlib –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, HTML –æ—Ç—á–µ—Ç –Ω–µ –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")
        return
    
    try:
        # –°–õ–û–í–ê–†–¨ –î–õ–Ø –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –¢–ï–•–ù–ò–ß–ï–°–ö–ò–• –ù–ê–ó–í–ê–ù–ò–ô –í –†–£–°–°–ö–ò–ï
        feature_name_mapping = {
            "tx_frequency": "–ß–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π",
            "days_since_last_tx": "–î–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏", 
            "total_tx_count": "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π",
            "region_index": "–†–µ–≥–∏–æ–Ω (–∏–Ω–¥–µ–∫—Å)",
            "customer_lifetime_days": "–í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫–ª–∏–µ–Ω—Ç–∞ (–¥–Ω–∏)",
            "avg_tx_amount": "–°—Ä–µ–¥–Ω—è—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏",
            "std_tx_amount": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—É–º–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"
        }
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
        feature_importance_russian = []
        for feature_name, importance_value in feature_importance:
            russian_name = feature_name_mapping.get(feature_name, feature_name)
            feature_importance_russian.append((russian_name, importance_value))
        
        # –°–û–ó–î–ê–ï–ú –ì–†–ê–§–ò–ö –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í
        plt.figure(figsize=(12, 8))
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        features = [x[0] for x in feature_importance_russian[:8]]
        importance = [x[1] for x in feature_importance_russian[:8]]
        
        # –†–∞–∑–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
        colors = ['#ff6b6b' if x > 0 else '#4ecdc4' for x in importance]
        
        # –°–æ–∑–¥–∞–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        bars = plt.barh(features, importance, color=colors, alpha=0.8)
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç)')
        plt.title('–¢–æ–ø-8 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞')
        plt.grid(axis='x', alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
        for bar, value in zip(bars, importance):
            plt.text(bar.get_width() + (0.01 if value > 0 else -0.03), 
                    bar.get_y() + bar.get_height()/2, 
                    f'{value:.4f}', 
                    ha='left' if value > 0 else 'right', 
                    va='center', 
                    fontsize=9)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–≥–µ–Ω–¥—É
        import matplotlib.patches as mpatches
        red_patch = mpatches.Patch(color='#ff6b6b', alpha=0.7, label='–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞')
        blue_patch = mpatches.Patch(color='#4ecdc4', alpha=0.7, label='–£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞')
        plt.legend(handles=[red_patch, blue_patch])
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64 –¥–ª—è HTML
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:
            chart_path = tmp_file.name
        
        plt.savefig(chart_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # –ß–∏—Ç–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –∫–∞–∫ base64 –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤ HTML
        with open(chart_path, "rb") as img_file:
            chart_base64 = base64.b64encode(img_file.read()).decode('utf-8')
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.unlink(chart_path)
        
        # –ì–ï–ù–ï–†–ê–¶–ò–Ø HTML –ö–û–î–ê
        html_content = f'''
        <!DOCTYPE html>
        <html lang="ru">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>–ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ - {datetime.now().strftime("%Y-%m-%d")}</title>
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f6fa; }}
                .container {{ max-width: 1200px; margin: 0 auto; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 20px; }}
                .card {{ background: white; padding: 25px; margin: 15px 0; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                .metrics {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
                .metric-box {{ background: #f8f9fa; padding: 15px; border-radius: 8px; text-align: center; }}
                .metric-value {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}
                .insight {{ background: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 10px 0; }}
                .feature-positive {{ color: #e74c3c; font-weight: bold; }}
                .feature-negative {{ color: #27ae60; font-weight: bold; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>–ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤</h1>
                    <p>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {datetime.now().strftime("%Y-%m-%d %H:%M")}</p>
                </div>
                
                <div class="card">
                    <h2>–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏</h2>
                    <div class="metrics">
                        <div class="metric-box">
                            <div class="metric-value">{metrics['auc']:.3f}</div>
                            <div>AUC Score</div>
                        </div>
                        <div class="metric-box">
                            <div class="metric-value">{metrics['f1']:.3f}</div>
                            <div>F1 Score</div>
                        </div>
                    </div>
                </div>
                
                <div class="card">
                    <h2>–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤</h2>
                    <img src="data:image/png;base64,{chart_base64}" alt="Feature Importance" style="max-width: 100%; border: 1px solid #ddd; border-radius: 5px;">
                    
                    <h3>–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤:</h3>
                    <ul>
        {"".join([f'<li><span class="{"feature-positive" if imp > 0 else "feature-negative"}">{feat}</span>: {imp:.4f}</li>' 
                 for feat, imp in feature_importance_russian[:5]])}
                    </ul>
                </div>
                
                <div class="card">
                    <h2>–ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h2>
                    <div class="insight">
                        {business_insights.replace(chr(10), '<br>').replace('===', '<h3>').replace('===', '</h3>')}
                    </div>
                </div>
                
                <div class="card">
                    <h2>üìã –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –æ—Ç—á–µ—Ç</h2>
                    <p><strong>–î–ª—è –±–∏–∑–Ω–µ—Å-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:</strong> –û—Ç–∫—Ä–æ–π—Ç–µ CSV —Ñ–∞–π–ª—ã –≤ Excel –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞</p>
                    <p><strong>–î–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–≤:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Parquet —Ñ–∞–π–ª—ã –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ Python/Jupyter</p>
                    <p><strong>–î–ª—è –¥–∞—à–±–æ—Ä–¥–æ–≤:</strong> –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Tableau/Power BI</p>
                </div>
            </div>
        </body>
        </html>
        '''
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –≤ S3
        html_rdd = spark.sparkContext.parallelize([html_content])
        html_rdd.coalesce(1).saveAsTextFile(save_path)
        
        print("   ‚úÖ HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ HTML –æ—Ç—á–µ—Ç–∞: {e}")

# =============================================================================
# –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –ë–ò–ó–ù–ï–°-–ò–ù–°–ê–ô–¢–û–í
# =============================================================================

def generate_business_insights(feature_importance, metrics):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–Ω—è—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    """
    insights = []
    insights.append("=== –ë–ò–ó–ù–ï–°-–ò–ù–°–ê–ô–¢–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ===")
    insights.append(f"–ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: AUC = {metrics['auc']:.3f}, F1 = {metrics['f1']:.3f}")
    insights.append("")
    insights.append("=== –ö–õ–Æ–ß–ï–í–´–ï –§–ê–ö–¢–û–†–´ –û–¢–¢–û–ö–ê ===")
    
    # –°–õ–û–í–ê–†–¨ –î–õ–Ø –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –ù–ê–ó–í–ê–ù–ò–ô –ü–†–ò–ó–ù–ê–ö–û–í
    feature_name_mapping = {
        "tx_frequency": "–ß–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π",
        "days_since_last_tx": "–î–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏", 
        "total_tx_count": "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π",
        "region_index": "–†–µ–≥–∏–æ–Ω",
        "customer_lifetime_days": "–í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫–ª–∏–µ–Ω—Ç–∞",
        "avg_tx_amount": "–°—Ä–µ–¥–Ω—è—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏",
        "std_tx_amount": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—É–º–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"
    }
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-3 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    top_features = feature_importance[:3]
    
    for feature, coef in top_features:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ
        feature_display = feature_name_mapping.get(feature, feature)
        
        if coef > 0:
            insights.append(f"{feature_display}: –£–í–ï–õ–ò–ß–ï–ù–ò–ï —ç—Ç–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –£–í–ï–õ–ò–ß–ò–í–ê–ï–¢ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞")
        else:
            insights.append(f"{feature_display}: –£–í–ï–õ–ò–ß–ï–ù–ò–ï —ç—Ç–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –£–ú–ï–ù–¨–®–ê–ï–¢ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞")
    
    insights.append("")
    insights.append("=== –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –ë–ò–ó–ù–ï–°–ê ===")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    for feature, coef in top_features:
        feature_lower = str(feature).lower()
        
        if "days_since_last_tx" in feature_lower and coef > 0:
            insights.append("–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É —Ä–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤")
        elif "tx_frequency" in feature_lower and coef < 0:
            insights.append("–°—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–æ–∫ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏")
        elif "avg_tx_amount" in feature_lower and coef < 0:
            insights.append("–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞")
        elif "customer_lifetime_days" in feature_lower and coef < 0:
            insights.append("–¶–µ–Ω–∏—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ - –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∏—è –∑–∞ –ª–æ—è–ª—å–Ω–æ—Å—Ç—å")
        elif "total_tx_count" in feature_lower and coef < 0:
            insights.append("–£–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
    
    insights.append("")
    insights.append("=== –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï –î–ï–ô–°–¢–í–ò–Ø ===")
    insights.append("1. –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É —É–¥–µ—Ä–∂–∞–Ω–∏—è –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º —Ä–∏—Å–∫–æ–º –æ—Ç—Ç–æ–∫–∞")
    insights.append("2. –°–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–∞–º —Ä–∏—Å–∫–∞") 
    insights.append("3. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
    
    return "\n".join(insights)

# =============================================================================
# –§–£–ù–ö–¶–ò–Ø –ü–û–î–ì–û–¢–û–í–ö–ò –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø ML
# =============================================================================

def prepare_features_adapted(df):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML-–º–æ–¥–µ–ª–∏
    –í–∫–ª—é—á–∞–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    Args:
        df: –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame
    
    Returns:
        feature_pipeline: –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        existing_features: —Å–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    # –ë–∞–∑–æ–≤—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    base_features = [
        "avg_tx_amount",           # –°—Ä–µ–¥–Ω—è—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        "total_tx_count",          # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        "days_since_last_tx",      # –î–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        "customer_lifetime_days",  # –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫–ª–∏–µ–Ω—Ç–∞ –≤ –¥–Ω—è—Ö
        "tx_frequency",            # –ß–∞—Å—Ç–æ—Ç–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        "std_tx_amount"            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Å—É–º–º—ã —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
    ]
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤ DataFrame –ø—Ä–∏–∑–Ω–∞–∫–∏
    existing_features = [c for c in base_features if c in df.columns]
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {existing_features}")
    
    stages = []  # –≠—Ç–∞–ø—ã –ø–∞–π–ø–ª–∞–π–Ω–∞
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ "region", –∫–æ–¥–∏—Ä—É–µ–º –µ–≥–æ
    if "region" in df.columns:
        region_indexer = StringIndexer(inputCol="region", outputCol="region_index")
        stages.append(region_indexer)
        existing_features.append("region_index")
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML-–º–æ–¥–µ–ª–∏
    assembler = VectorAssembler(
        inputCols=existing_features,
        outputCol="features",      # –í—ã—Ö–æ–¥–Ω–æ–π —Å—Ç–æ–ª–±–µ—Ü —Å –≤–µ–∫—Ç–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        handleInvalid="skip"       # –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    )
    stages.append(assembler)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_pipeline = Pipeline(stages=stages)
    return feature_pipeline, existing_features

# =============================================================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ML-–ü–ê–ô–ü–õ–ê–ô–ù–ê
# =============================================================================

def main():
    """
    –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ML-–ü–ê–ô–ü–õ–ê–ô–ù–ê –° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú –°–û–•–†–ê–ù–ï–ù–ò–ï–ú
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª ML: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞, –æ–±—É—á–µ–Ω–∏–µ, –æ—Ü–µ–Ω–∫–∞, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SparkSession - —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å–æ Spark
    spark = SparkSession.builder \
        .appName("Churn_ML_Fixed") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")  # –£–º–µ–Ω—å—à–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    print("=== –ó–∞–ø—É—Å–∫ ML-–ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º ===")
    
    try:
        # –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
        print("1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML...")
        df = spark.read.parquet("s3a://bucket-ml/processed/churn_features/")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö - —É–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        df_ml = df.filter(
            F.col("avg_tx_amount").isNotNull() &
            F.col("total_tx_count").isNotNull() &
            F.col("days_since_last_tx").isNotNull() &
            F.col("customer_lifetime_days").isNotNull()
        )
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {df_ml.count()} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –≠–¢–ê–ü 2: –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê –û–ë–£–ß–ê–Æ–©–£–Æ –ò –¢–ï–°–¢–û–í–£–Æ –í–´–ë–û–†–ö–ò
        df_train, df_test = df_ml.randomSplit([0.8, 0.2], seed=42)
        print(f"–†–∞–∑–º–µ—Ä train: {df_train.count()}, test: {df_test.count()}")
        
        # –≠–¢–ê–ü 3: –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
        feature_pipeline, feature_cols = prepare_features_adapted(df_train)
        feature_pipeline_model = feature_pipeline.fit(df_train)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        df_train_processed = feature_pipeline_model.transform(df_train)
        df_test_processed = feature_pipeline_model.transform(df_test)
        
        print("4. –û–±—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏...")
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        lr = LogisticRegression(
            featuresCol="features",     # –°—Ç–æ–ª–±–µ—Ü —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            labelCol="is_churned",      # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            maxIter=50,                 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
            regParam=0.01               # –ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        )
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        lr_model = lr.fit(df_train_processed)
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        predictions = lr_model.transform(df_test_processed)
        
        # –≠–¢–ê–ü 4: –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò
        # AUC (Area Under Curve) - –ø–ª–æ—â–∞–¥—å –ø–æ–¥ ROC –∫—Ä–∏–≤–æ–π
        evaluator_auc = BinaryClassificationEvaluator(labelCol="is_churned")
        auc = evaluator_auc.evaluate(predictions)
        
        # F1-score - –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã
        evaluator_f1 = MulticlassClassificationEvaluator(
            labelCol="is_churned",
            predictionCol="prediction", 
            metricName="f1"
        )
        f1 = evaluator_f1.evaluate(predictions)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞. AUC: {auc:.4f}, F1: {f1:.4f}")
        
        # –≠–¢–ê–ü 5: –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ú–û–î–ï–õ–ò
        feature_importance = interpret_model(lr_model, feature_cols)
        
        # –≠–¢–ê–ü 6: –ì–ï–ù–ï–†–ê–¶–ò–Ø –ò –°–û–•–†–ê–ù–ï–ù–ò–ï –ë–ò–ó–ù–ï–°-–ê–†–¢–ï–§–ê–ö–¢–û–í
        print("6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤...")
        
        metrics_dict = {"auc": auc, "f1": f1}
        business_insights = generate_business_insights(feature_importance, metrics_dict)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ S3
        base_path = save_business_artifacts_fixed(spark, df_ml, feature_importance, metrics_dict, business_insights)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML –æ—Ç—á–µ—Ç
        generate_html_report(spark, feature_importance, metrics_dict, business_insights, f"{base_path}/html_report/")
        
        # –≠–¢–ê–ü 7: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –ú–ï–¢–†–ò–ö
        print("7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        lr_model.write().overwrite().save("s3a://bucket-ml/models/churn_model_fixed/")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        metrics_data = [{
            'model': 'logistic_regression_fixed',
            'auc': float(auc),
            'f1': float(f1), 
            'timestamp': current_time,
            'features_count': len(feature_cols)
        }]
        
        # –°—Ö–µ–º–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        metrics_schema = StructType([
            StructField("model", StringType(), True),
            StructField("auc", FloatType(), True),
            StructField("f1", FloatType(), True),
            StructField("timestamp", StringType(), True),
            StructField("features_count", IntegerType(), True)
        ])
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        metrics_df = spark.createDataFrame(metrics_data, schema=metrics_schema)
        metrics_df.write.mode("append").json("s3a://bucket-ml/models/model_metrics/")
        
        print("ML-–ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω!")
        print(f"–í—Å–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {base_path}")
        print(f"–¢–æ–ø-3 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞:")
        for i, (feat, imp) in enumerate(feature_importance[:3], 1):
            print(f"   {i}. {feat}: {imp:.6f}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ ML-–ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
        import traceback
        traceback.print_exc()  # –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –æ—à–∏–±–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    finally:
        spark.stop()  # –í—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark —Å–µ—Å—Å–∏—é

# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É
if __name__ == "__main__":
    main()